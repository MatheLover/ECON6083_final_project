---
title: "Final Project"
author: "JIANG Rui Chan Yat Tin"
date: '2022-05-16'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Research Motivation

‘‘Long Live Keju! The Persistent Effects of China's Civil Examination System.’’ is the paper for our research project. Keju was the civil examination system in the Ming-Qing period of China for more than a hundred years now. Jinshi was the highest qualification that could be obtained in the Keju system. This academic paper suggests that the historical success in Keju has a positive impact on the contemporary human capital outcomes due to cultural transmission, educational infrastructure and the difference in social capital. It uses the variation in the density of Jinshi as the proxy regressor for this effect and accordingly to analyze the effect on the years of schooling in 2010. The scholars mention that there is a positive correlation between the density of Jinshi and the years of schooling in China today, a doubling of Jinshi per 10,000 population leads to an 8.5% increase in years of schooling in 2010. However, the Keju was the previous examination system for more than a hundred years now, the correlation between an ancient examination system (which has been replaced) and the years of schooling in China today can be doubtful. Moreover, the scholars acknowledge that the cause and the effect are most at salient at national level and their findings cannot be established as direct evidence to bear upon how the civil exam system affected China as a whole, which further motivates this paper to be worth analyzing.

## Research Question

The model suggested by the paper: yi = βkejui + γ1Xci+ γ2Xhi+ αp + εi , where i indexes a prefecture; the dependent variable yi means the average years of schooling measured at the prefecture level. αp denotes province fixed effects. Xci is a control variable of a number of covariates that may impact the contemporary years of schooling. Xhi is another control variable of historical changes that may correlate with the contemporary years of schooling. We are doubted the positive correlation between the density of Jinshi and the average years of schooling in China today, Lasso can help identify the regressors relevant to the dependent variable. Thus, we will use the Lasso technique for variable selection in this project to observe whether the results would be consistent with the paper. After this project, we hope our analysis can help to check the accuracy of the model suggested by the paper, which may assist to improve the understanding of Keju’s human capital outcomes in China today.

## Package
```{r cars}
suppressPackageStartupMessages({
  library(foreign)
  library(hdm)
  library(AER)
  library(kableExtra)
})
# install.packages("haven")
# install.packages("lfe")
library(haven)
library(lfe)
library(stargazer)
library(tidyverse)
library(devtools)
library(rlist)
#install.packages("rlist")
# devtools::install_github("https://github.com/rgiordan/zaminfluence/",
                           # ref="master",
                           # subdir="zaminfluence",
                           # force=TRUE)
library(zaminfluence)
# install.packages("plm")
library(plm)
library(glmnetUtils)
# install.packages("asbio")
# install.packages("rsq")
library(rsq)
library(readxl)
```

# Dataset Description 
To verify the causal relationship between human capital outcomes and jinshi density, we have the following relevant variables: 

1. provid (in GB code) -- https://sedac.ciesin.columbia.edu/downloads/docs/cddc/gbcodeplist.txt 
2. lneduyear -- average years of schooling in log scale 
3. lnjinshipop -- jinshi density 
4. lnjurenpop -- juren density
5. lnnspop -- Share of population with no education
6. lnmppop -- Share of population with elementary and secondary school education
7. lnpopqing -- population density 
8. lnofficialqing -- number of qing official in log scale 

We need to ensure that data is free from errors. So we check whether the above variables may contain outliers and other abnormal values. 

Potential Problems
1. Why is the number of qing officials can be 0.x sth if we use the min lnofficialqing to calculate? 
2. We are provided with jinshi density directly and cannot verify the raw data on jinshi populations as well as total populations in each prefecture and check whether jinshi density is calculated accurately. This is the conundrum we cannot solve and the following regressions are based on the assumption that jinshi and population data are 100% correct and jinshi density is calculated 100% accurately.  

```{r}
# Read and check data 
census <- read_dta("census10_final.dta")


# Check provid
n_distinct(unique(census$provid)) # 28 provinces
unique(census$provid) # making sense 
a <- aggregate(census, by=list(census$provid), FUN=length)
n_distinct(unique(census$prefid)) # 272 municipalities

# Check lneduyear
summary(census$lneduyear)
# 5.62 yrs
exp(min(census$lneduyear)) 
# 11.71 yrs approaching 12 (Since 1986, nine years of education has been mandatory for all Chinese children)
exp(max(census$lneduyear)) 

# Check jinshi density
# Note: Prefecture -- 府 e.g. 廣州府 in Canton Province, China
# jinshi density = # of jinshi at that prefecture / # of population at that prefecture
# variable meaning -- per 10000
# Min 0 Max 3.9586 
summary(census$lnjinshipop) 
lnjinshipop_sorted_desc <- as.data.frame(sort(census$lnjinshipop, decreasing = TRUE))
exp(min(census$lnjinshipop)) # 1 per 10000
exp(max(census$lnjinshipop)) # 52.384 per 10000

# Check juren density 
summary(census$lnjurenpop) 
lnjurenpop_sorted_desc <- as.data.frame(sort(census$lnjurenpop, decreasing = TRUE))
exp(min(census$lnjurenpop)) # 1
exp(max(census$lnjurenpop)) # 365.3902

# Check Shengyuan 
summary(census$lnshengyuanpop) 
lnshengyuanpop_sorted_desc <- as.data.frame(sort(census$lnshengyuanpop, decreasing = TRUE))
exp(min(census$lnshengyuanpop)) 
exp(max(census$lnshengyuanpop)) # 338.7812

# print colname 
relevant_var <- as.data.frame(colnames(census)[1:64])

# lnnspop Share of population with no education
summary(census$lnnspop) # min -0.3013 max 3.3877
exp(min(census$lnnspop)) # 0.7398606%
exp(max(census$lnnspop)) # 29.59724%

# lnmppop Share of population with elementary and secondary school education
summary(census$lnmppop) # min 3.764  max 4.436  
exp(min(census$lnmppop)) # 43.10818%
exp(max(census$lnmppop)) # 84.4415%

# population density 
summary(census$lnpopqing)
exp(min(census$lnpopqing)) # 0.24
exp(max(census$lnpopqing)) # 606 

# qing official 
summary(census$lnofficialqing)
# Why is number of people 0.x sth? 
exp(min(census$lnofficialqing)) # 0.27
exp(max(census$lnofficialqing)) # 288 


```

# Replication(original regression)
1. Mainly we try to replicate the most important baseline regressions in table 2

Since original codes are written in Stata, we need to convert them to R. To run regressions with fixed effects, we have many alternatives :

A. Use felm() method 

B. Use lm() method and factor() provid as FEs

We will implement based on felm() approach.  
```{r}
# baseline regressions -- table 2 
# Dependent var -- average years of schooling (logged) 
# table 2 column 1
table_2_col1 <- felm(lneduyear ~ lnjinshipop | provid ,data=census)
summary(table_2_col1,robust=TRUE)
a <- stargazer(table_2_col1,type="text", title="Table 2 Column 1")
```
The coefficient, SE, significance level are the same as table 2 column 1. 

```{r}
# table 2 column 2
table_2_col2 <- felm(lneduyear ~ lnjinshipop+lnnightlight+lncoastdist+tri| provid ,data=census)
summary(table_2_col2,robust=TRUE)
b <- stargazer(table_2_col2,type="text", title="Table 2 Column 2")
```
The coefficient, SE, significance level are roughly the same as table 2 column 2. 

```{r}
# table 2 column 3
# lnpopming result is strange a litte bit 
table_2_col3 <- felm(lneduyear ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates| provid ,data=census)
summary(table_2_col3,robust=TRUE)
stargazer(table_2_col3,type="text", title="Table 2 Column 3")
```
The coefficient, SE, significance level are roughly the same as table 2 column 3. 

```{r}
# table 2 column 4
table_2_col4 <- felm(lneduyear ~ lnjinshipopm+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates| provid ,data=census)
summary(table_2_col4,robust=TRUE)
stargazer(table_2_col4,type="text", title="Table 2 Column 4")
```
The coefficient, SE, significance level are roughly the same as table 2 column 4. 

```{r}
# Dependent variable -- Share of population with (* 100, logged)
table_2_col5 <- felm(lnnspop ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates| provid ,data=census)
summary(table_2_col5,robust=TRUE)
stargazer(table_2_col5,type="text", title="Table 2 Column 5")
```
The coefficient, SE, significance level are roughly the same as table 2 column 5. 

```{r}
table_2_col6 <- felm(lnmppop ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates| provid ,data=census)
summary(table_2_col6,robust=TRUE)
stargazer(table_2_col6,type="text", title="Table 2 Column 6")
```
The coefficient, SE, significance level are roughly the same as table 2 column 6

```{r}
table_2_col7 <- felm(lnhighpop ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates| provid ,data=census)
summary(table_2_col7,robust=TRUE)
stargazer(table_2_col7,type="text", title="Table 2 Column 7")
```
The coefficient, SE, significance level are roughly the same as table 2 column 7

```{r}
table_2_col8 <- felm(lnunipop ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates| provid ,data=census)
summary(table_2_col8,robust=TRUE)
stargazer(table_2_col8,type="text", title="Table 2 Column 8")

```
The coefficient, SE, significance level are roughly the same as table 2 column 8

IV Regression
```{r}
# 2SLS
# col 5
iv_5 <- ivreg(lneduyear ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)|bprvdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid),data=census)
iv_ROBUST_5 <- coeftest(iv_5,vcov=vcovHC(iv_5, type = "HC0"))
stargazer(iv_ROBUST_5, type="text")

# col 6
iv_6 <- ivreg(lneduyear ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)+rvrdist|rvrdist+bprvdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid),data=census)
iv_ROBUST_6 <- coeftest(iv_6,vcov=vcovHC(iv_6, type = "HC0"))
stargazer(iv_ROBUST_6, type="text")

# first stage
iv_1st_6 <- lm(lnjinshipop ~ rvrdist+bprvdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid),data=census)
iv_1st_6_rob <- coeftest(iv_1st_6,vcov=vcovHC(iv_1st_6, type = "HC0"))
stargazer(iv_1st_6_rob, type="text")

# col 7
iv_7 <- ivreg(lnhighpop ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)+rvrdist|rvrdist+bprvdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid),data=census)
iv_ROBUST_7 <- coeftest(iv_7,vcov=vcovHC(iv_7, type = "HC0"))
stargazer(iv_ROBUST_7, type="text")

# col 8
iv_8 <- ivreg(lnunipop ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)+rvrdist| lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)+ rvrdist + bprvdist,data=census)
iv_ROBUST_8 <- coeftest(iv_8,vcov=vcovHC(iv_8, type = "HC0"))
stargazer(iv_ROBUST_8, type="text")

# first stage
iv_1st_8 <- lm(lnjinshipop ~ rvrdist+bprvdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid),data=census)
iv_1st_8_rob <- coeftest(iv_1st_8,vcov=vcovHC(iv_1st_8, type = "HC0"))
stargazer(iv_1st_8_rob, type="text")

# Reduced Form
# col 1 
rf_1 <- lm(lneduyear ~ bprvdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid),data=census)
rf_1 <- coeftest(rf_1,vcov=vcovHC(rf_1, type = "HC0")) # disapperance of bprvdist if we run this line
stargazer(rf_1, type="text")

# col 2
rf_2 <- lm(lneduyear ~ bprvdist+rvrdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid),data=census)
rf_2 <- coeftest(rf_2,vcov=vcovHC(rf_2, type = "HC0"))
stargazer(rf_2, type="text")

# col 3
rf_3 <- lm(lnhighpop ~ bprvdist+rvrdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid),data=census)
rf_3 <- coeftest(rf_3,vcov=vcovHC(rf_3, type = "HC0")) # disapperance of bprvdist if we run this line
stargazer(rf_3, type="text")

# col 4 
rf_4 <- lm(lnhighpop ~ bprvdist+rvrdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid),data=census)
rf_4 <- coeftest(rf_4,vcov=vcovHC(rf_4, type = "HC0")) # disapperance of bprvdist if we run this line
stargazer(rf_4, type="text")


```





# Result check
1. To detect if there is any p-hacking issue, we perform some checks. Basically, we adopt advice from (https://michaelwiebe.com/blog/2021/01/amip) and drop 1% of the total observations for checking. In this case, since we have 273 observations, we drop 3 random observations and check whether the coefficient on jinshi density is statistically significant at 1% level.   

Result: it turns out if we randomly drop 3 observations each time and we repeat 1000 times, the coefficient on jinshi density is always 1% statistically significant each time. 

```{r}
# 1. Significance Check
counter <- 0
for (i in 1:1000){
  dropped_ob <- c(sample(1:274, 3, replace=TRUE))
  new_census <- census[-dropped_ob,]
  
  table_2_col1_v2 <- felm(lneduyear ~ lnjinshipop | provid,data=new_census)
  
  if(summary(table_2_col1_v2)$coefficients[1,4] < 0.01){
    counter <- counter + 1
  }
  
}
counter # 1000

counter <- 0
for (i in 1:1000){
  dropped_ob <- c(sample(1:274, 3, replace=TRUE))
  new_census <- census[-dropped_ob,]
  
  table_2_col1_v2 <- felm(lneduyear ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates| provid,data=new_census)
  
  if(summary(table_2_col1_v2)$coefficients[1,4] < 0.01){
    counter <- counter + 1
  }
  
}
counter # 1000

```

# ML Approach 
We use adaptive lasso to select relevant variables and then perform regressions.
```{r}

# Table 2 col 1 - 4
baselinemodel <- glmnet(lneduyear~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census,standardize=FALSE,intercept=TRUE,alpha=1,lambda=0.00,)
coef((baselinemodel))
w=1/abs(coef(baselinemodel)[-1])
# w[1]=0
w[2:29] = 0
X=model.matrix(lneduyear~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census)[,-1]
head(X,3)
y <- census$lneduyear
cv_lambda_lasso <- cv.glmnet(lneduyear~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census,standardize=FALSE,alpha = 1,penalty.factor=w, intercept=TRUE)
coef(cv_lambda_lasso,s=cv_lambda_lasso$lambda.1se)
post_lasso <- glm(lneduyear~lnjinshipop+factor(provid)+lnnightlight,data=census)
ML_result <- coef(summary(post_lasso))



```

```{r}
# Table 2 col 5 - 8
# col 5
col5 <- glmnet(lnnspop~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census,standardize=FALSE,intercept=TRUE,alpha=1,lambda=0.00)
w=1/abs(coef(col5)[-1])
# w[1]=0
w[2:29] = 0
X=model.matrix(lnnspop~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census)[,-1]
head(X,3)
y <- census$lnnspop
cv_lambda_lasso <- cv.glmnet(lnnspop~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census,standardize=FALSE,alpha = 1,penalty.factor=w, intercept=TRUE)
coef(cv_lambda_lasso,s=cv_lambda_lasso$lambda.1se)
post_lasso_5 <- glm(lnnspop~lnjinshipop+factor(provid)+lnnightlight,data=census)
ML_col5 <- coef(summary(post_lasso_5))






```

```{r}
# col 6
col6 <- glmnet(lnmppop~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census,standardize=FALSE,intercept=TRUE,alpha=1,lambda=0.00)
w=1/abs(coef(col6)[-1])
# w[1]=0
w[2:29] = 0
X=model.matrix(lnmppop~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census)[,-1]
head(X,3)
y <- census$lnmppop
cv_lambda_lasso <- cv.glmnet(lnmppop~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census,standardize=FALSE,alpha = 1,penalty.factor=w, intercept=TRUE)
coef(cv_lambda_lasso,s=cv_lambda_lasso$lambda.1se)
post_lasso_6 <- glm(lnmppop~lnjinshipop+factor(provid)+tri,data=census)
coef(summary(post_lasso_6))
```

```{r}
# col 7
col7 <- glmnet(lnhighpop~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census,standardize=FALSE,intercept=TRUE,alpha=1,lambda=0.00)
w=1/abs(coef(col7)[-1])
# w[1]=0
w[2:29] = 0
X=model.matrix(lnhighpop~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census)[,-1]
head(X,3)
y <- census$lnhighpop
cv_lambda_lasso <- cv.glmnet(lnhighpop~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census,standardize=FALSE,alpha = 1,penalty.factor=w, intercept=TRUE)
coef(cv_lambda_lasso,s=cv_lambda_lasso$lambda.1se)
post_lasso_7 <- glm(lnhighpop~lnjinshipop+factor(provid)+lnnightlight,data=census)
coef(summary(post_lasso_7))
```

```{r}
# col 8
col8 <- glmnet(lnunipop~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census,standardize=FALSE,intercept=TRUE,alpha=1,lambda=0.00)
w=1/abs(coef(col8)[-1])
# w[1]=0
w[2:29] = 0
X=model.matrix(lnunipop~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census)[,-1]
head(X,3)
y <- census$lnunipop
cv_lambda_lasso <- cv.glmnet(lnunipop~lnjinshipop+factor(provid)+lnnightlight+lncoastdist+tri+suitability+urbanrates+lnpopming,data=census,standardize=FALSE,alpha = 1,penalty.factor=w, intercept=TRUE)
coef(cv_lambda_lasso,s=cv_lambda_lasso$lambda.1se)
post_lasso_8 <- glm(lnunipop~lnjinshipop+factor(provid)+lnnightlight+tri,data=census)
ML_col8 <- coef(summary(post_lasso_8))
```

# Lasso IV coefficients -- 2SLS
```{r}
lasso.iv_5 <- rlassoIV(lneduyear ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)|bprvdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid),data=census,select.X = TRUE, select.Z = TRUE)
summary(lasso.iv_5)
lasso.iv_5$coefficients

# 1st stage
First_5 <- rlasso(lnjinshipop ~ bprvdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid) + bprvdist,data=census)
First_5 <- which(First_5$index==TRUE)
First_5

lasso.iv_6 <- rlassoIV(lneduyear ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)+rvrdist|bprvdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)+rvrdist,data=census,select.X = TRUE, select.Z = TRUE)
summary(lasso.iv_6)
lasso.iv_6$coefficients

# 1st stage 
First_6 <- rlasso(lnjinshipop ~ bprvdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)+rvrdist+ bprvdist,data=census)
which(First_6$index==TRUE)
First_6

lasso.iv_7 <- rlassoIV(lnhighpop ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)+rvrdist|rvrdist+bprvdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid),data=census,select.X = TRUE, select.Z = TRUE)
summary(lasso.iv_7)
lasso.iv_7$coefficients

# 1st stage
First_7 <- rlasso(lnjinshipop ~ rvrdist+bprvdist+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)+ bprvdist,data=census)
which(First_7$index==TRUE)
First_7

lasso.iv_8 <- rlassoIV(lnunipop ~ lnjinshipop+lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)+rvrdist| lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)+ rvrdist + bprvdist,data=census,select.X=TRUE, select.Z=TRUE)
summary(lasso.iv_8)
lasso.iv_8$coefficients

# 1st stage
First_8 <- rlasso(lnjinshipop ~ lnnightlight+lncoastdist+tri+suitability+lnpopqing+urbanrates+as.factor(provid)+ rvrdist + bprvdist,data=census)
which(First_8$index==TRUE)
First_8

```

# Table 2 comparisons 
Column 3
```{r}
# Traditional estimate col 3
result_col3 <- summary(table_2_col3,robust=TRUE)$coef[,1:2]

# ML estimate col 3
ML_result_2 <- ML_result[c(2,30),1:2]
ML_result_2 <- as.data.frame(ML_result_2)
for (i in 1:5){
  ML_result_2[nrow(ML_result_2)+1,] <- c(0,0)
}
colnames(ML_result_2) <- colnames(result_col3) 
rownames(ML_result_2) <- rownames(result_col3) 

# cbind 
combined_1 <- cbind(result_col3, ML_result_2)
colnames(combined_1) <- c("Traditional Estimates", "Traditional SE", "ML Estimate", "ML SE")
combined_1
```
Table 2 Average Years of Schooling: Traditional vs ML methods

Column 5 and 8
```{r}
result_col5 <- summary(table_2_col5, robust=TRUE)$coef[,1:2]
ML_col5_v2 <- ML_col5[c(2,30),1:2]
ML_col5_2 <- as.data.frame(ML_col5_v2)
for (i in 1:(nrow(result_col5) - nrow(ML_col5_2))){
  ML_col5_2[nrow(ML_col5_2)+1,] <- c(0,0)
}
colnames(ML_col5_2) <- colnames(result_col5) 
rownames(ML_col5_2) <- rownames(result_col5) 

# cbind 
combined_2 <- cbind(result_col5, ML_col5_2)
colnames(combined_2) <- c("Traditional Estimates", "Traditional SE", "ML Estimate", "ML SE")
combined_2
```
Table 2 Share of Population without education 


```{r}
result_col8 <- summary(table_2_col8, robust=TRUE)$coef[,1:2]
ML_col8_v2 <- ML_col8[c(2,30),1:2]
ML_col8_2 <- as.data.frame(ML_col8_v2)
for (i in 1:(nrow(result_col8) - nrow(ML_col8_2))){
  ML_col8_2[nrow(ML_col8_2)+1,] <- c(0,0)
}
colnames(ML_col8_2) <- colnames(result_col8) 
rownames(ML_col8_2) <- rownames(result_col8) 

# cbind 
combined_3 <- cbind(result_col8, ML_col8_2)
colnames(combined_3) <- c("Traditional Estimates", "Traditional SE", "ML Estimate", "ML SE")
combined_3
```
Table 2 Share of Population with University Education 


# Table 5 comparison(2sls)
```{r}
# Traditional IV col 6 and 8
sample_iv <- c(-0.088)
sample_iv <- as.data.frame((sample_iv))
colnames(sample_iv) <- c("Traditional coefficient")
rownames(sample_iv) <- c("bprvdist")
col6_iv <- iv_ROBUST_6[c(2),1]
col6_iv <- as.data.frame(col6_iv)
rownames(col6_iv) <- c("lnjinshipop")
colnames(col6_iv) <- c("Traditional coefficient")
col6_iv <- rbind(sample_iv, col6_iv)



# ML
# col 6 
df <- as.data.frame(First_6$coefficients)
df_1 <- as.data.frame(df[2,1])
rownames(df_1) <- c("bprvdist")
colnames(df_1) <- c("ML coefficient")

# jinshi coefficient 
df_2 <- as.data.frame(lasso.iv_6$coefficients)
colnames(df_2) <- c("ML coefficient")
rownames(df_2) <- c("lnjinshipop")

# rbind 
rbind_6 <- rbind(df_1, df_2)  

# Final table 
table_5_1 <- cbind(col6_iv, rbind_6)
table_5_1
```


```{r}
sample_iv <- c(-0.088)
sample_iv <- as.data.frame((sample_iv))
colnames(sample_iv) <- c("Traditional coefficient")
rownames(sample_iv) <- c("bprvdist")
col8_iv <- iv_ROBUST_8[c(2),1]
col8_iv <- as.data.frame(col8_iv)
rownames(col8_iv) <- c("lnjinshipop")
colnames(col8_iv) <- c("Traditional coefficient")
col8_iv <- rbind(sample_iv, col8_iv)



# ML
# col 8
df <- as.data.frame(First_8$coefficients)
df_8 <- as.data.frame(df[36,1])
rownames(df_8) <- c("bprvdist")
colnames(df_8) <- c("ML coefficient")

# jinshi coefficient 
df_9 <- as.data.frame(lasso.iv_8$coefficients)
colnames(df_9) <- c("ML coefficient")
rownames(df_9) <- c("lnjinshipop")

# rbind 
rbind_9 <- rbind(df_8, df_9)  

# Final table 
table_8_1 <- cbind(col8_iv, rbind_9)
table_8_1

```





